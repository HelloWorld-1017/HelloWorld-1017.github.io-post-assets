{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f78bf74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T03:17:54.738379Z",
     "start_time": "2024-07-27T03:17:53.667760Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv_block = self.build_conv_block(dim)\n",
    "\n",
    "    def build_conv_block(self, dim):\n",
    "        conv_block = []\n",
    "\n",
    "        conv_block += [nn.ReflectionPad2d(1)]\n",
    "\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True),\n",
    "                       nn.InstanceNorm2d(dim),\n",
    "                       nn.ReLU(True)]\n",
    "\n",
    "        conv_block += [nn.ReflectionPad2d(1)]\n",
    "\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True),\n",
    "                       nn.InstanceNorm2d(dim)]\n",
    "\n",
    "        return nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self, input_nc=3, output_nc=3, ngf=64, n_blocks=9): # <3> \n",
    "\n",
    "        assert(n_blocks >= 0)\n",
    "        super(ResNetGenerator, self).__init__()\n",
    "\n",
    "        self.input_nc = input_nc\n",
    "        self.output_nc = output_nc\n",
    "        self.ngf = ngf\n",
    "\n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=True),\n",
    "                 nn.InstanceNorm2d(ngf),\n",
    "                 nn.ReLU(True)]\n",
    "\n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**i\n",
    "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n",
    "                                stride=2, padding=1, bias=True),\n",
    "                      nn.InstanceNorm2d(ngf * mult * 2),\n",
    "                      nn.ReLU(True)]\n",
    "\n",
    "        mult = 2**n_downsampling\n",
    "        for i in range(n_blocks):\n",
    "            model += [ResNetBlock(ngf * mult)]\n",
    "\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**(n_downsampling - i)\n",
    "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
    "                                         kernel_size=3, stride=2,\n",
    "                                         padding=1, output_padding=1,\n",
    "                                         bias=True),\n",
    "                      nn.InstanceNorm2d(int(ngf * mult / 2)),\n",
    "                      nn.ReLU(True)]\n",
    "\n",
    "        model += [nn.ReflectionPad2d(3)]\n",
    "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
    "        model += [nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, input): # <3>\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0aef3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T03:17:55.759978Z",
     "start_time": "2024-07-27T03:17:54.739383Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Instantiate the class `ResNetGenerator` with a set of pretrained parameters\n",
    "netG = ResNetGenerator()\n",
    "model_path = 'horse2zebra_0.4.0.pth'\n",
    "model_data = torch.load(model_path)\n",
    "netG.load_state_dict(model_data)\n",
    "\n",
    "# Put the model in eval mode\n",
    "netG.eval()\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load an image and preprocess it\n",
    "img = Image.open('horse.jpg') # img.size: (1500, 1220)\n",
    "preprocess = transforms.Compose([transforms.Resize(256),\n",
    "                                transforms.ToTensor()])\n",
    "img_t = preprocess(img) # img_t.size(): torch.Size([3, 256, 314])\n",
    "batch_t = torch.unsqueeze(img_t, 0) # batch_t.size(): torch.Size([1, 3, 256, 314])\n",
    "\n",
    "# Send the preprocessed image to Generator\n",
    "batch_out = netG(batch_t) # batch_out.size(): torch.Size([1, 3, 256, 316])\n",
    "\n",
    "# Convert back to an image\n",
    "out_t = (batch_out.data.squeeze() + 1.0) / 2.0 # batch_out.data.squeeze().size(): torch.Size([3, 256, 316])\n",
    "out_img = transforms.ToPILImage()(out_t)\n",
    "out_img.save('zebra.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3386066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T03:17:55.763822Z",
     "start_time": "2024-07-27T03:17:55.760981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.1.weight\n",
      "model.1.bias\n",
      "model.4.weight\n",
      "model.4.bias\n",
      "model.7.weight\n",
      "model.7.bias\n",
      "model.10.conv_block.1.weight\n",
      "model.10.conv_block.1.bias\n",
      "model.10.conv_block.5.weight\n",
      "model.10.conv_block.5.bias\n",
      "model.11.conv_block.1.weight\n",
      "model.11.conv_block.1.bias\n",
      "model.11.conv_block.5.weight\n",
      "model.11.conv_block.5.bias\n",
      "model.12.conv_block.1.weight\n",
      "model.12.conv_block.1.bias\n",
      "model.12.conv_block.5.weight\n",
      "model.12.conv_block.5.bias\n",
      "model.13.conv_block.1.weight\n",
      "model.13.conv_block.1.bias\n",
      "model.13.conv_block.5.weight\n",
      "model.13.conv_block.5.bias\n",
      "model.14.conv_block.1.weight\n",
      "model.14.conv_block.1.bias\n",
      "model.14.conv_block.5.weight\n",
      "model.14.conv_block.5.bias\n",
      "model.15.conv_block.1.weight\n",
      "model.15.conv_block.1.bias\n",
      "model.15.conv_block.5.weight\n",
      "model.15.conv_block.5.bias\n",
      "model.16.conv_block.1.weight\n",
      "model.16.conv_block.1.bias\n",
      "model.16.conv_block.5.weight\n",
      "model.16.conv_block.5.bias\n",
      "model.17.conv_block.1.weight\n",
      "model.17.conv_block.1.bias\n",
      "model.17.conv_block.5.weight\n",
      "model.17.conv_block.5.bias\n",
      "model.18.conv_block.1.weight\n",
      "model.18.conv_block.1.bias\n",
      "model.18.conv_block.5.weight\n",
      "model.18.conv_block.5.bias\n",
      "model.19.weight\n",
      "model.19.bias\n",
      "model.22.weight\n",
      "model.22.bias\n",
      "model.26.weight\n",
      "model.26.bias\n"
     ]
    }
   ],
   "source": [
    "for key in model_data.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5a73585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T03:17:57.004725Z",
     "start_time": "2024-07-27T03:17:55.764827Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\PythonLearning\\venv\\Lib\\site-packages\\torch\\onnx\\_internal\\jit_utils.py:307: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ..\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "G:\\PythonLearning\\venv\\Lib\\site-packages\\torch\\onnx\\symbolic_helper.py:1515: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'instance_norm' is set to train=True. Exporting with train=True.\n",
      "  warnings.warn(\n",
      "G:\\PythonLearning\\venv\\Lib\\site-packages\\torch\\onnx\\utils.py:702: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ..\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'netG.pth' at http://localhost:8080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\PythonLearning\\venv\\Lib\\site-packages\\torch\\onnx\\utils.py:1208: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ..\\torch\\csrc\\jit\\passes\\onnx\\constant_fold.cpp:181.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 8080)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import netron\n",
    "import torch.onnx\n",
    "\n",
    "netG = ResNetGenerator()\n",
    "model_path = 'horse2zebra_0.4.0.pth'\n",
    "model_data = torch.load(model_path)\n",
    "netG.load_state_dict(model_data)\n",
    "netG.eval()\n",
    "\n",
    "x = torch.randn(1, 3, 256, 314)\n",
    "modelFile = \"netG.pth\"\n",
    "torch.onnx.export(netG, x, modelFile)\n",
    "netron.start(modelFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64b9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
